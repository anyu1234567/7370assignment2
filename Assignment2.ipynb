{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4420ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "453bb4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cad9a1e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip install gym[accept-rom-license]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0370d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip uninstall tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e424b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keras-rl2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bff4e83",
   "metadata": {},
   "source": [
    "from ale_py import ALEInterface\n",
    "\n",
    "ale = ALEInterface()\n",
    "ALEInterface.isSupportedROM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59690e10",
   "metadata": {},
   "source": [
    "!ale-import-roms roms/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21a3c1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gym keras-rl2 gym[atari]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5c69580",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c42b13be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width,channels =env.observation_space.shape \n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6e9de902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "700b5872",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "07196610",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8a32cfaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4afa1636",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d99dbd50",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(height, width, channels, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1d80f642",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "194472d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "401d3848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import LinearAnnealedPolicy, EpsGreedyQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8fdb366e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "# initial lr 0.7\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.7, clipnorm=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3e8e88a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = LinearAnnealedPolicy(EpsGreedyQPolicy(), attr='eps', value_max=1., value_min=.01, value_test=.2, nb_steps=100)\n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100 , gamma=.8\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8b31f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial epsilon :1.0\n",
    "dqn = build_agent(model, actions)\n",
    "#dqn.compile(Adam(epsilon=0.7))\n",
    "dqn.compile(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7ea96762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "   99/5000: episode: 1, duration: 2.654s, episode steps:  99, steps per second:  37, episode reward: 10.000, mean reward:  0.101 [ 0.000, 10.000], mean action: 2.071 [0.000, 5.000],  loss: --, mean_q: --, mean_eps: --\n",
      "  198/5000: episode: 2, duration: 99.151s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.949 [2.000, 4.000],  loss: inf, mean_q: 5702238046675566592.000000, mean_eps: 0.010000\n",
      "  297/5000: episode: 3, duration: 102.739s, episode steps:  99, steps per second:   1, episode reward: 15.000, mean reward:  0.152 [ 0.000,  5.000], mean action: 3.960 [0.000, 4.000],  loss: inf, mean_q: 7507417715632123904.000000, mean_eps: 0.010000\n",
      "  396/5000: episode: 4, duration: 99.459s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7471613352259362816.000000, mean_eps: 0.010000\n",
      "  495/5000: episode: 5, duration: 99.066s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7480390581438138368.000000, mean_eps: 0.010000\n",
      "  594/5000: episode: 6, duration: 99.746s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.990 [2.000, 5.000],  loss: inf, mean_q: 7480343063655315456.000000, mean_eps: 0.010000\n",
      "  693/5000: episode: 7, duration: 99.941s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7474949359387459584.000000, mean_eps: 0.010000\n",
      "  792/5000: episode: 8, duration: 103.286s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.980 [3.000, 4.000],  loss: inf, mean_q: 7476719306559905792.000000, mean_eps: 0.010000\n",
      "  891/5000: episode: 9, duration: 99.879s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.980 [2.000, 4.000],  loss: inf, mean_q: 7454919117370230784.000000, mean_eps: 0.010000\n",
      "  990/5000: episode: 10, duration: 101.068s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.929 [0.000, 4.000],  loss: inf, mean_q: 7464961856667131904.000000, mean_eps: 0.010000\n",
      " 1089/5000: episode: 11, duration: 100.941s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7456819012379047936.000000, mean_eps: 0.010000\n",
      " 1188/5000: episode: 12, duration: 103.429s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.990 [3.000, 4.000],  loss: inf, mean_q: 7442536100892142592.000000, mean_eps: 0.010000\n",
      " 1287/5000: episode: 13, duration: 102.150s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7469901929092195328.000000, mean_eps: 0.010000\n",
      " 1386/5000: episode: 14, duration: 102.024s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.980 [2.000, 4.000],  loss: inf, mean_q: 7466253699533432832.000000, mean_eps: 0.010000\n",
      " 1485/5000: episode: 15, duration: 103.389s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7452162030879602688.000000, mean_eps: 0.010000\n",
      " 1584/5000: episode: 16, duration: 108.477s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.960 [0.000, 4.000],  loss: inf, mean_q: 7423055531392466944.000000, mean_eps: 0.010000\n",
      " 1683/5000: episode: 17, duration: 103.296s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7446868770886106112.000000, mean_eps: 0.010000\n",
      " 1782/5000: episode: 18, duration: 103.115s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.970 [1.000, 4.000],  loss: inf, mean_q: 7450409320495503360.000000, mean_eps: 0.010000\n",
      " 1881/5000: episode: 19, duration: 105.275s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.960 [0.000, 4.000],  loss: inf, mean_q: 7449266450348588032.000000, mean_eps: 0.010000\n",
      " 1980/5000: episode: 20, duration: 104.898s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.980 [2.000, 4.000],  loss: inf, mean_q: 7444862484244578304.000000, mean_eps: 0.010000\n",
      " 2079/5000: episode: 21, duration: 103.096s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.899 [0.000, 4.000],  loss: inf, mean_q: 7458782684615365632.000000, mean_eps: 0.010000\n",
      " 2178/5000: episode: 22, duration: 101.362s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.010 [4.000, 5.000],  loss: inf, mean_q: 7448601545680590848.000000, mean_eps: 0.010000\n",
      " 2277/5000: episode: 23, duration: 102.189s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7465268276119760896.000000, mean_eps: 0.010000\n",
      " 2376/5000: episode: 24, duration: 104.940s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.929 [0.000, 4.000],  loss: inf, mean_q: 7461083818071986176.000000, mean_eps: 0.010000\n",
      " 2475/5000: episode: 25, duration: 101.656s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.010 [4.000, 5.000],  loss: inf, mean_q: 7455466502015102976.000000, mean_eps: 0.010000\n",
      " 2574/5000: episode: 26, duration: 101.291s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7434503218873015296.000000, mean_eps: 0.010000\n",
      " 2673/5000: episode: 27, duration: 102.320s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.980 [1.000, 5.000],  loss: inf, mean_q: 7441522689909764096.000000, mean_eps: 0.010000\n",
      " 2772/5000: episode: 28, duration: 102.028s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7426636279813346304.000000, mean_eps: 0.010000\n",
      " 2871/5000: episode: 29, duration: 102.283s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7441817231304960000.000000, mean_eps: 0.010000\n",
      " 2970/5000: episode: 30, duration: 102.777s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7448754449987009536.000000, mean_eps: 0.010000\n",
      " 3069/5000: episode: 31, duration: 101.998s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.020 [4.000, 5.000],  loss: inf, mean_q: 7442956658536678400.000000, mean_eps: 0.010000\n",
      " 3168/5000: episode: 32, duration: 100.928s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.020 [4.000, 5.000],  loss: inf, mean_q: 7431742223007711232.000000, mean_eps: 0.010000\n",
      " 3267/5000: episode: 33, duration: 101.220s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7462030169951248384.000000, mean_eps: 0.010000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 3366/5000: episode: 34, duration: 100.943s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.960 [0.000, 4.000],  loss: inf, mean_q: 7459593418954357760.000000, mean_eps: 0.010000\n",
      " 3465/5000: episode: 35, duration: 104.212s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.990 [1.000, 5.000],  loss: inf, mean_q: 7448408209332951040.000000, mean_eps: 0.010000\n",
      " 3564/5000: episode: 36, duration: 100.610s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7443391609787976704.000000, mean_eps: 0.010000\n",
      " 3663/5000: episode: 37, duration: 101.785s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.939 [1.000, 4.000],  loss: inf, mean_q: 7440654458886964224.000000, mean_eps: 0.010000\n",
      " 3762/5000: episode: 38, duration: 102.364s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7440620129690586112.000000, mean_eps: 0.010000\n",
      " 3861/5000: episode: 39, duration: 101.183s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7448489495450160128.000000, mean_eps: 0.010000\n",
      " 3960/5000: episode: 40, duration: 102.567s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.970 [1.000, 4.000],  loss: inf, mean_q: 7462724578181257216.000000, mean_eps: 0.010000\n",
      " 4059/5000: episode: 41, duration: 102.002s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7457035960461240320.000000, mean_eps: 0.010000\n",
      " 4158/5000: episode: 42, duration: 102.087s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7445541266082059264.000000, mean_eps: 0.010000\n",
      " 4257/5000: episode: 43, duration: 102.157s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.929 [0.000, 4.000],  loss: inf, mean_q: 7447330510238881792.000000, mean_eps: 0.010000\n",
      " 4356/5000: episode: 44, duration: 103.389s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.990 [3.000, 4.000],  loss: inf, mean_q: 7453956256157284352.000000, mean_eps: 0.010000\n",
      " 4455/5000: episode: 45, duration: 102.186s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.960 [0.000, 4.000],  loss: inf, mean_q: 7446081726024912896.000000, mean_eps: 0.010000\n",
      " 4554/5000: episode: 46, duration: 102.966s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7451504284143365120.000000, mean_eps: 0.010000\n",
      " 4653/5000: episode: 47, duration: 100.815s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.990 [3.000, 4.000],  loss: inf, mean_q: 7444788244997347328.000000, mean_eps: 0.010000\n",
      " 4752/5000: episode: 48, duration: 101.307s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.010 [4.000, 5.000],  loss: inf, mean_q: 7444918409404191744.000000, mean_eps: 0.010000\n",
      " 4851/5000: episode: 49, duration: 100.639s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 4.000 [4.000, 4.000],  loss: inf, mean_q: 7463581630835842048.000000, mean_eps: 0.010000\n",
      " 4950/5000: episode: 50, duration: 101.751s, episode steps:  99, steps per second:   1, episode reward: 10.000, mean reward:  0.101 [ 0.000,  5.000], mean action: 3.960 [1.000, 5.000],  loss: inf, mean_q: 7467578244541027328.000000, mean_eps: 0.010000\n",
      "done, took 5056.725 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dcba981a48>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2,nb_max_episode_steps=99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62d185d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "Episode 1: reward: 270.000, steps: 720\n",
      "Episode 2: reward: 270.000, steps: 705\n",
      "Episode 3: reward: 270.000, steps: 715\n",
      "Episode 4: reward: 270.000, steps: 722\n",
      "Episode 5: reward: 270.000, steps: 735\n",
      "Episode 6: reward: 270.000, steps: 719\n",
      "Episode 7: reward: 270.000, steps: 713\n",
      "Episode 8: reward: 270.000, steps: 711\n",
      "Episode 9: reward: 270.000, steps: 714\n",
      "Episode 10: reward: 270.000, steps: 717\n",
      "Episode 11: reward: 270.000, steps: 729\n",
      "Episode 12: reward: 270.000, steps: 718\n",
      "Episode 13: reward: 270.000, steps: 719\n",
      "Episode 14: reward: 270.000, steps: 727\n",
      "Episode 15: reward: 270.000, steps: 714\n",
      "Episode 16: reward: 270.000, steps: 723\n",
      "Episode 17: reward: 270.000, steps: 731\n",
      "Episode 18: reward: 270.000, steps: 703\n",
      "Episode 19: reward: 270.000, steps: 716\n",
      "Episode 20: reward: 270.000, steps: 720\n",
      "Episode 21: reward: 270.000, steps: 727\n",
      "Episode 22: reward: 270.000, steps: 713\n",
      "Episode 23: reward: 270.000, steps: 709\n",
      "Episode 24: reward: 270.000, steps: 727\n",
      "Episode 25: reward: 270.000, steps: 725\n",
      "Episode 26: reward: 270.000, steps: 710\n",
      "Episode 27: reward: 270.000, steps: 715\n",
      "Episode 28: reward: 270.000, steps: 727\n",
      "Episode 29: reward: 270.000, steps: 716\n",
      "Episode 30: reward: 270.000, steps: 709\n",
      "Episode 31: reward: 270.000, steps: 715\n",
      "Episode 32: reward: 270.000, steps: 709\n",
      "Episode 33: reward: 270.000, steps: 713\n",
      "Episode 34: reward: 270.000, steps: 720\n",
      "Episode 35: reward: 270.000, steps: 713\n",
      "Episode 36: reward: 270.000, steps: 717\n",
      "Episode 37: reward: 270.000, steps: 719\n",
      "Episode 38: reward: 270.000, steps: 734\n",
      "Episode 39: reward: 270.000, steps: 728\n",
      "Episode 40: reward: 270.000, steps: 718\n",
      "Episode 41: reward: 270.000, steps: 716\n",
      "Episode 42: reward: 270.000, steps: 721\n",
      "Episode 43: reward: 270.000, steps: 722\n",
      "Episode 44: reward: 270.000, steps: 726\n",
      "Episode 45: reward: 270.000, steps: 713\n",
      "Episode 46: reward: 270.000, steps: 722\n",
      "Episode 47: reward: 270.000, steps: 726\n",
      "Episode 48: reward: 270.000, steps: 715\n",
      "Episode 49: reward: 270.000, steps: 725\n",
      "Episode 50: reward: 270.000, steps: 728\n",
      "Episode 51: reward: 270.000, steps: 728\n",
      "Episode 52: reward: 270.000, steps: 722\n",
      "Episode 53: reward: 270.000, steps: 716\n",
      "Episode 54: reward: 270.000, steps: 724\n",
      "Episode 55: reward: 270.000, steps: 713\n",
      "Episode 56: reward: 270.000, steps: 718\n",
      "Episode 57: reward: 270.000, steps: 710\n",
      "Episode 58: reward: 270.000, steps: 719\n",
      "Episode 59: reward: 270.000, steps: 727\n",
      "Episode 60: reward: 270.000, steps: 725\n",
      "Episode 61: reward: 270.000, steps: 722\n",
      "Episode 62: reward: 270.000, steps: 723\n",
      "Episode 63: reward: 270.000, steps: 709\n",
      "Episode 64: reward: 270.000, steps: 714\n",
      "Episode 65: reward: 270.000, steps: 712\n",
      "Episode 66: reward: 270.000, steps: 717\n",
      "Episode 67: reward: 270.000, steps: 721\n",
      "Episode 68: reward: 270.000, steps: 733\n",
      "Episode 69: reward: 270.000, steps: 719\n",
      "Episode 70: reward: 270.000, steps: 719\n",
      "Episode 71: reward: 270.000, steps: 707\n",
      "Episode 72: reward: 270.000, steps: 722\n",
      "Episode 73: reward: 270.000, steps: 718\n",
      "Episode 74: reward: 270.000, steps: 728\n",
      "Episode 75: reward: 270.000, steps: 713\n",
      "Episode 76: reward: 270.000, steps: 720\n",
      "Episode 77: reward: 270.000, steps: 721\n",
      "Episode 78: reward: 270.000, steps: 722\n",
      "Episode 79: reward: 270.000, steps: 727\n",
      "Episode 80: reward: 270.000, steps: 716\n",
      "Episode 81: reward: 270.000, steps: 730\n",
      "Episode 82: reward: 270.000, steps: 719\n",
      "Episode 83: reward: 270.000, steps: 721\n",
      "Episode 84: reward: 270.000, steps: 716\n",
      "Episode 85: reward: 270.000, steps: 695\n",
      "Episode 86: reward: 270.000, steps: 724\n",
      "Episode 87: reward: 270.000, steps: 719\n",
      "Episode 88: reward: 270.000, steps: 722\n",
      "Episode 89: reward: 270.000, steps: 730\n",
      "Episode 90: reward: 270.000, steps: 715\n",
      "Episode 91: reward: 270.000, steps: 710\n",
      "Episode 92: reward: 270.000, steps: 707\n",
      "Episode 93: reward: 270.000, steps: 708\n",
      "Episode 94: reward: 270.000, steps: 725\n",
      "Episode 95: reward: 270.000, steps: 712\n",
      "Episode 96: reward: 270.000, steps: 718\n",
      "Episode 97: reward: 270.000, steps: 706\n",
      "Episode 98: reward: 270.000, steps: 710\n",
      "Episode 99: reward: 270.000, steps: 710\n",
      "Episode 100: reward: 270.000, steps: 712\n",
      "270.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cde3e9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('d:/TensorflowModule/SavedWeights/base_line_dif_alpha_gamma/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fd5ee38",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tensorflow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4852/1250110539.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mhelp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtensorflow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tensorflow' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02473714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(tf4gym)",
   "language": "python",
   "name": "tf4gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
