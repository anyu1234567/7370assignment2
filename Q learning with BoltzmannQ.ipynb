{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0bd9c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\An\\anaconda3\\envs\\tf4gym\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import random\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "height, width,channels =env.observation_space.shape \n",
    "actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92e4752b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NOOP', 'FIRE', 'RIGHT', 'LEFT', 'RIGHTFIRE', 'LEFTFIRE']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.unwrapped.get_action_meanings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "118f341f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Convolution2D\n",
    "from tensorflow.keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e50768",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(height, width, channels, actions):\n",
    "    model = Sequential()\n",
    "    model.add(Convolution2D(32, (8,8), strides=(4,4), activation='relu', input_shape=(3,height, width, channels)))\n",
    "    model.add(Convolution2D(64, (4,4), strides=(2,2), activation='relu'))\n",
    "    model.add(Convolution2D(64, (3,3), activation='relu'))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(512, activation='relu'))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Dense(actions, activation='linear'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d10fe45",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1047168d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 3, 51, 39, 32)     6176      \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 3, 24, 18, 64)     32832     \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 22, 16, 64)     36928     \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 67584)             0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               34603520  \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 1542      \n",
      "=================================================================\n",
      "Total params: 34,812,326\n",
      "Trainable params: 34,812,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model(height, width, channels, actions)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "66129920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl.agents import DQNAgent\n",
    "from rl.memory import SequentialMemory\n",
    "from rl.policy import BoltzmannQPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c872056f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras \n",
    "lr_schedule = keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=.7,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.01)\n",
    "optimizer = keras.optimizers.SGD(learning_rate=lr_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7e94558",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_agent(model, actions):\n",
    "    policy = BoltzmannQPolicy()\n",
    "    \n",
    "    memory = SequentialMemory(limit=1000, window_length=3)\n",
    "    dqn = DQNAgent(model=model, memory=memory, policy=policy,\n",
    "                  enable_dueling_network=True, dueling_type='avg', \n",
    "                   nb_actions=actions, nb_steps_warmup=100 , gamma=.8\n",
    "                  )\n",
    "    return dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "753e0dff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_13676/993787854.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdqn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_agent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdqn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "dqn = build_agent(model, actions)\n",
    "dqn.compile(Adam(epsilon=1.))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "287aa465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n",
      "WARNING:tensorflow:From C:\\Users\\An\\anaconda3\\envs\\tf4gym\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "  926/5000: episode: 1, duration: 756.923s, episode steps: 926, steps per second:   1, episode reward: 140.000, mean reward:  0.151 [ 0.000, 20.000], mean action: 2.529 [0.000, 5.000],  loss: 0.564472, mean_q: -1.128461\n",
      " 1557/5000: episode: 2, duration: 611.165s, episode steps: 631, steps per second:   1, episode reward: 115.000, mean reward:  0.182 [ 0.000, 25.000], mean action: 2.585 [0.000, 5.000],  loss: 0.581945, mean_q: -1.159402\n",
      " 2183/5000: episode: 3, duration: 596.627s, episode steps: 626, steps per second:   1, episode reward: 90.000, mean reward:  0.144 [ 0.000, 25.000], mean action: 2.436 [0.000, 5.000],  loss: 0.419707, mean_q: -1.376690\n",
      " 3081/5000: episode: 4, duration: 847.573s, episode steps: 898, steps per second:   1, episode reward: 325.000, mean reward:  0.362 [ 0.000, 200.000], mean action: 2.575 [0.000, 5.000],  loss: 6.701224, mean_q: -1.323718\n",
      " 3765/5000: episode: 5, duration: 641.490s, episode steps: 684, steps per second:   1, episode reward: 95.000, mean reward:  0.139 [ 0.000, 25.000], mean action: 2.667 [0.000, 5.000],  loss: 16.430189, mean_q: -0.897289\n",
      " 4172/5000: episode: 6, duration: 380.600s, episode steps: 407, steps per second:   1, episode reward: 30.000, mean reward:  0.074 [ 0.000, 10.000], mean action: 2.568 [0.000, 5.000],  loss: 14.143588, mean_q: -1.329230\n",
      " 4845/5000: episode: 7, duration: 628.193s, episode steps: 673, steps per second:   1, episode reward: 110.000, mean reward:  0.163 [ 0.000, 30.000], mean action: 2.443 [0.000, 5.000],  loss: 0.546602, mean_q: -1.386747\n",
      "done, took 4606.776 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1718f812c88>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn.fit(env, nb_steps=5000, visualize=False, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9e2395e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.save_weights('d:/TensorflowModule/SavedWeights/base_line_nonGreedy/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf15612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e25a9571",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing for 100 episodes ...\n",
      "WARNING:tensorflow:From C:\\Users\\An\\anaconda3\\envs\\tf4gym\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "Episode 1: reward: 80.000, steps: 975\n",
      "Episode 2: reward: 40.000, steps: 671\n",
      "Episode 3: reward: 140.000, steps: 972\n",
      "Episode 4: reward: 125.000, steps: 1123\n",
      "Episode 5: reward: 125.000, steps: 987\n",
      "Episode 6: reward: 75.000, steps: 716\n",
      "Episode 7: reward: 300.000, steps: 822\n",
      "Episode 8: reward: 85.000, steps: 796\n",
      "Episode 9: reward: 55.000, steps: 557\n",
      "Episode 10: reward: 80.000, steps: 907\n",
      "Episode 11: reward: 220.000, steps: 1091\n",
      "Episode 12: reward: 75.000, steps: 688\n",
      "Episode 13: reward: 55.000, steps: 716\n",
      "Episode 14: reward: 60.000, steps: 667\n",
      "Episode 15: reward: 15.000, steps: 633\n",
      "Episode 16: reward: 55.000, steps: 695\n",
      "Episode 17: reward: 60.000, steps: 706\n",
      "Episode 18: reward: 50.000, steps: 487\n",
      "Episode 19: reward: 115.000, steps: 1051\n",
      "Episode 20: reward: 105.000, steps: 1009\n",
      "Episode 21: reward: 15.000, steps: 536\n",
      "Episode 22: reward: 55.000, steps: 638\n",
      "Episode 23: reward: 65.000, steps: 644\n",
      "Episode 24: reward: 145.000, steps: 1194\n",
      "Episode 25: reward: 20.000, steps: 380\n",
      "Episode 26: reward: 45.000, steps: 619\n",
      "Episode 27: reward: 75.000, steps: 673\n",
      "Episode 28: reward: 155.000, steps: 1217\n",
      "Episode 29: reward: 30.000, steps: 521\n",
      "Episode 30: reward: 50.000, steps: 709\n",
      "Episode 31: reward: 35.000, steps: 527\n",
      "Episode 32: reward: 50.000, steps: 734\n",
      "Episode 33: reward: 125.000, steps: 1294\n",
      "Episode 34: reward: 100.000, steps: 662\n",
      "Episode 35: reward: 85.000, steps: 1036\n",
      "Episode 36: reward: 50.000, steps: 657\n",
      "Episode 37: reward: 30.000, steps: 393\n",
      "Episode 38: reward: 15.000, steps: 600\n",
      "Episode 39: reward: 85.000, steps: 907\n",
      "Episode 40: reward: 60.000, steps: 511\n",
      "Episode 41: reward: 110.000, steps: 1210\n",
      "Episode 42: reward: 70.000, steps: 1135\n",
      "Episode 43: reward: 15.000, steps: 635\n",
      "Episode 44: reward: 55.000, steps: 515\n",
      "Episode 45: reward: 35.000, steps: 674\n",
      "Episode 46: reward: 145.000, steps: 657\n",
      "Episode 47: reward: 30.000, steps: 801\n",
      "Episode 48: reward: 70.000, steps: 625\n",
      "Episode 49: reward: 140.000, steps: 1252\n",
      "Episode 50: reward: 85.000, steps: 702\n",
      "Episode 51: reward: 100.000, steps: 796\n",
      "Episode 52: reward: 35.000, steps: 632\n",
      "Episode 53: reward: 55.000, steps: 709\n",
      "Episode 54: reward: 115.000, steps: 683\n",
      "Episode 55: reward: 75.000, steps: 528\n",
      "Episode 56: reward: 150.000, steps: 1083\n",
      "Episode 57: reward: 70.000, steps: 688\n",
      "Episode 58: reward: 50.000, steps: 685\n",
      "Episode 59: reward: 45.000, steps: 671\n",
      "Episode 60: reward: 35.000, steps: 494\n",
      "Episode 61: reward: 200.000, steps: 1173\n",
      "Episode 62: reward: 35.000, steps: 394\n",
      "Episode 63: reward: 45.000, steps: 555\n",
      "Episode 64: reward: 50.000, steps: 640\n",
      "Episode 65: reward: 15.000, steps: 528\n",
      "Episode 66: reward: 110.000, steps: 667\n",
      "Episode 67: reward: 240.000, steps: 1345\n",
      "Episode 68: reward: 50.000, steps: 844\n",
      "Episode 69: reward: 75.000, steps: 722\n",
      "Episode 70: reward: 45.000, steps: 554\n",
      "Episode 71: reward: 45.000, steps: 664\n",
      "Episode 72: reward: 105.000, steps: 1177\n",
      "Episode 73: reward: 65.000, steps: 507\n",
      "Episode 74: reward: 135.000, steps: 1136\n",
      "Episode 75: reward: 85.000, steps: 1069\n",
      "Episode 76: reward: 35.000, steps: 400\n",
      "Episode 77: reward: 65.000, steps: 699\n",
      "Episode 78: reward: 210.000, steps: 1223\n",
      "Episode 79: reward: 30.000, steps: 653\n",
      "Episode 80: reward: 85.000, steps: 647\n",
      "Episode 81: reward: 50.000, steps: 620\n",
      "Episode 82: reward: 60.000, steps: 590\n",
      "Episode 83: reward: 35.000, steps: 397\n",
      "Episode 84: reward: 15.000, steps: 385\n",
      "Episode 85: reward: 90.000, steps: 726\n",
      "Episode 86: reward: 95.000, steps: 793\n",
      "Episode 87: reward: 70.000, steps: 800\n",
      "Episode 88: reward: 75.000, steps: 645\n",
      "Episode 89: reward: 65.000, steps: 897\n",
      "Episode 90: reward: 60.000, steps: 668\n",
      "Episode 91: reward: 130.000, steps: 917\n",
      "Episode 92: reward: 290.000, steps: 1097\n",
      "Episode 93: reward: 120.000, steps: 996\n",
      "Episode 94: reward: 60.000, steps: 551\n",
      "Episode 95: reward: 35.000, steps: 623\n",
      "Episode 96: reward: 50.000, steps: 626\n",
      "Episode 97: reward: 75.000, steps: 531\n",
      "Episode 98: reward: 60.000, steps: 932\n",
      "Episode 99: reward: 60.000, steps: 801\n",
      "Episode 100: reward: 85.000, steps: 971\n",
      "79.95\n"
     ]
    }
   ],
   "source": [
    "scores = dqn.test(env, nb_episodes=100, visualize=False)\n",
    "print(np.mean(scores.history['episode_reward']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba55b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "52e45f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn.load_weights('d:/TensorflowModule/SavedWeights/base_line_nonGreedy/dqn_weights.h5f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7775628",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python(tf4gym)",
   "language": "python",
   "name": "tf4gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
